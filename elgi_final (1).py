# -*- coding: utf-8 -*-
"""ELGi_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-RD7fJvIqLSLCuRq1ksiFTSNU0tQP7e_

# Import Libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import math
import warnings
warnings.filterwarnings('ignore')

"""# Import dataset from Kaggle"""

!pip install kaggle

from google.colab import files

uploaded = files.upload()



!ls -lha kaggle.json

!pip install -q kaggle

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

import os

os.environ['KAGGLE_CONFIG_DIR'] = '/content'

!chmod 600 /root/.kaggle/kaggle.json

!pwd

!kaggle datasets download -d darkside92/turbofan-predictive-maintenance-nasa

!unzip turbofan-predictive-maintenance-nasa.zip

"""# Exploratory Data Analysis"""

data_train=pd.read_csv('train_FD001.txt',sep=" ",header=None)
data_train

df= data_train.drop(data_train.iloc[:, 26:28],axis = 1)
df

col_names = ['engine', 'cycle', 'setting_1', 'setting_2', 'setting_3',"(Fan inlet temperature) (◦R)",
"(LPC outlet temperature) (◦R)",
"(HPC outlet temperature) (◦R)",
"(LPT outlet temperature) (◦R)",
"(Fan inlet Pressure) (psia)",
"(bypass-duct pressure) (psia)",
"(HPC outlet pressure) (psia)",
"(Physical fan speed) (rpm)",
"(Physical core speed) (rpm)",
"(Engine pressure ratio(P50/P2)",
"(HPC outlet Static pressure) (psia)",
"(Ratio of fuel flow to Ps30) (pps/psia)",
"(Corrected fan speed) (rpm)",
"(Corrected core speed) (rpm)",
"(Bypass Ratio) ",
"(Burner fuel-air ratio)",
"(Bleed Enthalpy)",
"(Required fan speed)",
"(Required fan conversion speed)",
"(High-pressure turbines Cool air flow)",
"(Low-pressure turbines Cool air flow)" ]
df.columns = col_names
df.describe()

df
df.to_csv('nasa_train.csv')

df['engine'].unique()

df.info()

df.columns

fig, axes = plt.subplots(3, 2, figsize=(40, 20))

fig.suptitle('Check for Outliers')
sns.set(style='whitegrid', rc={"grid.linewidth": 0.1})
sns.set_context("paper", font_scale=3)
sns.boxplot(ax=axes[0, 0], x=df['setting_1']).set_xlabel('Setting_1',fontsize=30)
sns.boxplot(ax=axes[0, 1], x=df['(HPC outlet temperature) (◦R)']).set_xlabel('HPC',fontsize=30)
sns.boxplot(ax=axes[1, 0], x=df['(LPC outlet temperature) (◦R)']).set_xlabel('LPC',fontsize=30)
sns.boxplot(ax=axes[1, 1], x=df['(LPT outlet temperature) (◦R)']).set_xlabel('LPT',fontsize=30)
sns.boxplot(ax=axes[2, 0], x=df['(Bypass Ratio) ']).set_xlabel('BPR',fontsize=30)
sns.boxplot(ax=axes[2, 1], x=df['(Bleed Enthalpy)']).set_xlabel('Bleed Enthalpy',fontsize=30)

_, ax = plt.subplots(3, 2, figsize= (40, 10))
sns.distplot( x=df['(HPC outlet temperature) (◦R)'],ax=ax[0,0]).set_ylabel('HPC',fontsize=30)
sns.distplot( x=df['(LPC outlet temperature) (◦R)'],ax=ax[0,1]).set_ylabel('LPC',fontsize=30)
sns.distplot( x=df['(LPT outlet temperature) (◦R)'],ax=ax[1,0]).set_ylabel('LPT',fontsize=30)
sns.distplot( x=df['(Bypass Ratio) '],ax=ax[1,1]).set_ylabel('BPR',fontsize=30)
sns.distplot( x=df['(Bleed Enthalpy)'],ax=ax[2,0]).set_ylabel('Bleed enthalpy',fontsize=30)
sns.distplot( x=df['setting_1'],ax=ax[2,1]).set_ylabel('Bleed enthalpy',fontsize=30)

plt.style.use('dark_background')
plt.figure(figsize=(20,50))
ax=df.groupby('engine')['cycle'].max().plot(kind='barh',width=0.8, stacked=True,align='center',rot=0)
plt.title('Engines LifeTime before RUL',fontweight='bold',size=35)
plt.xlabel('Cycle Time',fontweight='bold',size=30)
plt.xticks(size=25)
plt.ylabel('Engine',fontweight='bold',size=30)
plt.yticks(size=25)
plt.grid(True)

plt.show()

df_RUL = df.groupby(['engine']).agg({'cycle':'max'})
df_RUL.rename(columns={'cycle':'life'},inplace=True)
df_RUL.head()

df=df.merge(df_RUL,how='left',on=['engine'])
df

x=df[df["cycle"] == df["life"]]
x.columns
df['(Bleed Enthalpy)'].value_counts()

fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.set_xlabel('engine')
ax1.set_ylabel('bleed enthalphy', color=color)
ax1.plot(x['engine'], x['(Bleed Enthalpy)'], color=color)

ax1.tick_params(axis='y', labelcolor=color)

ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis

color = 'tab:blue'
ax2.set_ylabel('life', color=color)  # we already handled the x-label with ax1
ax2.bar(x['engine'], x['life'], color=color)
ax2.tick_params(axis='y', labelcolor=color)
ax1.tick_params(
    axis='x',          # changes apply to the x-axis
    which='both',      # both major and minor ticks are affected
    bottom=False,      # ticks along the bottom edge are off
    top=False,         # ticks along the top edge are off
    labelbottom=False)
sns.set(font_scale = 2)

sns.distplot(x['life'])

"""Life Vs Bleed Enthalphy



"""

from scipy.stats import pearsonr

corr, _ = pearsonr(df['(Bleed Enthalpy)'], df['life'])
print('Pearsons correlation: %.3f' % corr)

from scipy.stats import spearmanr


corr, pval = spearmanr(df['(Bleed Enthalpy)'], df['life'])

print("Spearman's correlation coefficient:", corr)
print("p-value:", pval)

summary1=df.groupby('engine')['life','(Bleed Enthalpy)'].mean().sort_values(by='life',ascending=False)
summary1.head()

"""Null Hypothesis-life is not affected by the Bypass Ratio.
Alternate Hypothesis-life is affected by the Bypass Ratio.

p-value < 0.05.Therefore reject the Null Hypothesis. Accept the Alternate Hypothesis.
Life is affacted by the Bypass Ratio
"""

sns.lineplot(x=df['(Bleed Enthalpy)'],y=df['life'],).set(xlabel=None,ylabel=None)

final_df=df
final_df['RUL']=df['life']-df['cycle']
final_df.drop(columns=['life'],inplace=True)
final_df

_, ax = plt.subplots(3, 2, figsize= (40, 10))
sns.scatterplot(x='RUL', y = '(Bypass Ratio) ' , data=df, ax=ax[0,0]).set(ylabel='BPR')
sns.scatterplot(x='RUL', y = '(Low-pressure turbines Cool air flow)' , data=df, ax=ax[0,1]).set(ylabel='cool air flow')
sns.scatterplot(x='RUL', y = '(HPC outlet temperature) (◦R)' , data=df, ax=ax[1,0]).set(ylabel='HPC')
sns.scatterplot(x='RUL', y = '(LPT outlet temperature) (◦R)' , data=df, ax=ax[1,1]).set(ylabel='LPT')
sns.scatterplot(x='RUL', y = '(LPC outlet temperature) (◦R)' , data=df, ax=ax[2,0]).set(ylabel='LPC')
sns.scatterplot(x='RUL', y = '(Ratio of fuel flow to Ps30) (pps/psia)' , data=df, ax=ax[2,1]).set(ylabel='Fuel ratio')

final_df['RUL'].mean()

#df['RUL'][df['RUL']>108]=108
#df.head()

df2=pd.read_csv('RUL_FD001.txt',sep=" ",header=None)
df2.columns=["True_RUL","null"]
df2

df2.drop(["null"],axis=1,inplace=True)
df2['engine']=df2.index+1
df2

test=pd.read_csv('test_FD001.txt',sep=" ",header=None)
test

test= test.drop(test.iloc[:, 26:28],axis = 1)
test

col_names = ['engine', 'cycle',
'setting_1', 'setting_2', 'setting_3',
 "(Fan inlet temperature) (◦R)",
"(LPC outlet temperature) (◦R)",
"(HPC outlet temperature) (◦R)",
"(LPT outlet temperature) (◦R)",
"(Fan inlet Pressure) (psia)",
"(bypass-duct pressure) (psia)",
"(HPC outlet pressure) (psia)",
"(Physical fan speed) (rpm)",
"(Physical core speed) (rpm)",
"(Engine pressure ratio(P50/P2)",
"(HPC outlet Static pressure) (psia)",
"(Ratio of fuel flow to Ps30) (pps/psia)",
"(Corrected fan speed) (rpm)",
"(Corrected core speed) (rpm)",
"(Bypass Ratio) ",
"(Burner fuel-air ratio)",
"(Bleed Enthalpy)",
"(Required fan speed)",
"(Required fan conversion speed)",
"(High-pressure turbines Cool air flow)",
"(Low-pressure turbines Cool air flow)" ]
test.columns = col_names
test.to_csv('nasa_test.csv')

rul = pd.DataFrame(test.groupby('engine')['cycle'].max()).reset_index()
rul.columns = ['engine', 'max']
rul

df2['life']=df2['True_RUL']+rul['max']
df2

test=test.merge(df2,on=['engine'],how='left')

test["remaining_cycle"]=test["life"]-test["cycle"]
test

final_test=test.drop(columns=['True_RUL', 'life'])
final_test

final_df['RUL'].value_counts()

nonunique_columns=[]
for i in df.select_dtypes(include=np.number):
    if final_df[i].nunique()==1:
        nonunique_columns.append(i)
print(nonunique_columns)

final_df.drop(columns=nonunique_columns, inplace=True)
final_df.drop(columns=['(Physical fan speed) (rpm)','(Physical core speed) (rpm)','(bypass-duct pressure) (psia)'],inplace=True)
final_df
final_df.to_csv('new_data.csv')

Selected_Features = []
import statsmodels.api as sm

def backward_regression(X, y, initial_list=[], threshold_out=0.05, verbose=True):
    """To select feature with Backward Stepwise Regression

    Args:
        X -- features values
        y -- target variable
        initial_list -- features header
        threshold_out -- pvalue threshold of features to drop
        verbose -- true to produce lots of logging output

    Returns:
        list of selected features for modeling
    """
    included = list(X.columns)
    while True:
        changed = False
        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()
        # use all coefs except intercept
        pvalues = model.pvalues.iloc[1:]
        worst_pval = pvalues.max()  # null if pvalues is empty
        if worst_pval > threshold_out:
            changed = True
            worst_feature = pvalues.idxmax()
            included.remove(worst_feature)
            if verbose:
                print(f"worst_feature : {worst_feature}, {worst_pval} ")
        if not changed:
            break
    Selected_Features.append(included)
    print(f"\nSelected Features:\n{Selected_Features[0]}")


# Application of the backward regression function on our training data
X = final_df.iloc[:,1:-1]
y = final_df.iloc[:,-1]
backward_regression(X, y)

Selected_Features

feature_names = Selected_Features[0]

Selected_Features[0]

X = final_df[feature_names]
y = final_df.iloc[:,-1]

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
X = sc.fit_transform(X)

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

x_test

"""# Model Building &Evaluation for NPDD Engineer"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import  mean_squared_error,r2_score, mean_absolute_percentage_error
model = LinearRegression()
model.fit(x_train, y_train)

y_pred = model.predict(x_test)

y_test.shape

#sol_df=pd.DataFrame()
#sol_df['prediction']=y_pred
#sol_df['actual']=y_test
#sol_df

y_test

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
mape = mean_absolute_percentage_error(y_test, y_pred)
print(f"Mean squared error: {mse}")
print(f"Root mean squared error: {rmse}")
print(f"R-squared score: {r2}")
print(f"mean_absolute_percentage_error: {mape}")

from sklearn.ensemble import RandomForestRegressor

model=RandomForestRegressor()
model.fit(x_train, y_train)

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
print(f"Mean squared error: {mse}")
print(f"Root mean squared error: {rmse}")
print(f"R-squared score: {r2}")

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error

def train_and_evaluate_polynomial_regression(degree, alpha=0.1):
    # Create polynomial features
    poly_features = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly_train = poly_features.fit_transform(x_train)

    # Create and fit a Ridge regression model
    ridge_reg = Ridge(alpha=alpha)
    ridge_reg.fit(X_poly_train, y_train)

    # Transform test data for evaluation
    X_poly_test = poly_features.transform(x_test)

    # Make predictions
    y_pred = ridge_reg.predict(X_poly_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)

# Try different degrees and regularization parameters
degrees = [1, 2, 3]
alphas = [0.1, 1.0, 10.0]

for degree in degrees:
    for alpha in alphas:
        train_and_evaluate_polynomial_regression(degree, alpha)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
r2

"""# Model building & evaluation for Sales Engineer"""

cycle=100
final_df['label'] = final_df['RUL'].apply(lambda x: 1 if x <= cycle else 0)
final_test['label'] = final_test['remaining_cycle'].apply(lambda x: 1 if x <= cycle else 0)

final_df

final_df.drop(columns=['engine','setting_1','RUL'],inplace=True)

final_df

X = final_df.drop(columns=["label"]) # Independent Variables
y = final_df["label"] # Target Variable

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X= scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)

from tensorflow.keras.utils import to_categorical
import tensorflow

# Model Init
model = Sequential() # Empty model with no layers or inputs
model.add(Input(14,)) # Input Layer
model.add(Dense(units=10, activation="relu"))
model.add(Dense(units=4, activation="relu"))
model.add(Dense(units = 1,activation="softmax")) # Output Layer

#model.compile(optimizer="Adam",loss= "mean_squared_error", metrics="mean_absolute_percentage_error")
model.compile(optimizer=Adam(learning_rate=0.005),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x=X_train, y=y_train, epochs=50, batch_size=100, validation_data=(X_test, y_test))

history=model.history.history
val_loss=history['val_loss']
train_loss=history['loss']
val_acc=history['val_accuracy']
train_acc=history['accuracy']

plt.plot(val_loss)
plt.plot(train_loss)
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.legend(['validation','train'])
plt.grid()
plt.show()

preds = model.predict(X_test)
preds

from sklearn.model_selection import RandomizedSearchCV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 400, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5]
# Minimum number of samples required at each leaf node
min_samples_leaf = [2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]

# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

print(random_grid)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix
# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestClassifier(random_state = 42)
# Random search of parameters, using 3 fold cross validation,
# search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,
                              n_iter = 10, scoring='neg_mean_absolute_error',
                              cv = 3, verbose=2, random_state=42, n_jobs=-1,
                              return_train_score=True)

# Fit the random search model
rf2=rf_random.fit(X_train, y_train);

rf2.best_params_

y_pred=rf2.predict(X_test)

accuracy_score(y_pred,y_test)

from sklearn.metrics import classification_report

report = classification_report(y_test, y_pred)

print(report)

final_df
final_df.to_csv('to_azure.csv')

import joblib

filename = 'best_model.sav'
joblib.dump(rf2, filename)

